# 为何选择 Krawl ?

## 动机

几个月之前，我买了人生第一台 NAS 设备。渐渐的开始把越来越多云端数据存储在本地设备中。

数据多了就需要有一个优秀的管理工具来管理这些数据。你可以轻松从网络上找到不少优秀的工具：

- 用于管理视频的：[Plex](https://www.plex.tv/)、[Emby](https://emby.media/)
- 用于管理漫画的：[Mmmc](https://mmmc.blackcater.org)
- 等等...

如果想要在这些工具中获得更好的界面展示，就需要为资源附带上一些元数据。元数据通常也只能从相应的网站中爬取。

一些强大的管理工具可能自身支持帮你爬取相应资源的元数据，但这些工具通常只支持从特定网站爬取原信息，然而有时候爬取的数据不一定是你想要的。因此，你或许不得不自己“亲力亲为”地编辑或专门为其写一个爬虫脚本。

编写一个非通用爬虫程序或许不困难，但是费时，你需要考虑很多细节，比如：错误处理、并发控制、失败重试、数据存储、信息整理等等细枝末节。这些细节将会耗费你大量的时间和精力。

通常编写的程序都是为了解决一个特定的问题，且使用困难，不易维护和分发。因此，我想要一个写一个通用的爬虫程序，它能够帮我解决这些头疼的问题。

## Krawl 和 X 区别是什么？

如果你上网寻找爬虫框架帮你解决你的需求，你应该能看到如下的一些选择：

### Scrapy

[Scrapy](https://scrapy.org/) 是著名的爬虫框架，但它是基于 Python 开发的。因此，如果你对 Python 不熟悉，你可能需要花费不少时间去学习它。

最头疼的是，Scrapy 仅仅只能爬取对 SEO 友好的网站，对于一些 CSR 网站，它无能为力。你需要寻找 Python 相关的库做对应处理。

### Crawlee

[Crawlee](https://crawlee.dev/) 是一个基于 Node.js 的爬虫框架。Krawl 的设计理念很多都是来自于 Crawlee。

Crawlee 由于支持 Headless 模式，你也可以使用它爬取 CSR 网站。但 Crawlee 有一些 “low level”，虽然框架本身提供了丰富的功能和工具，但是你需要自己去组合这些工具来完成你的需求。

Krawl 对 Crawlee 理念进行简化和封装，提供了更加简单的 API和强大的插件系统。

Krawl 仅只是 Headless 模式以获得统一的开发体验。使用和开发体验一直是我们最求的第一目标，因此我们也提供了许多额外能力来获得更好的使用和开发体验。
